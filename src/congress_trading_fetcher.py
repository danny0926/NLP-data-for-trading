"""
Congress Trading Fetcher - æ•´åˆç‰ˆæœ¬
çµåˆ burd5/congress_stock_trading å°ˆæ¡ˆçš„æ¶æ§‹èˆ‡ä½ ç¾æœ‰çš„è³‡æ–™æŠ“å–åŠŸèƒ½
"""

import pandas as pd
import requests
import logging
import sqlite3
import json
from datetime import datetime, timedelta
from curl_cffi import requests as cf_requests
from bs4 import BeautifulSoup
import os
import sys

# Ensure src is in path
sys.path.append(os.path.dirname(os.path.abspath(__file__)))
from database import DB_PATH, generate_hash

# Setup Logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger("CongressTradingFetcher")


class CongressTradingFetcher:
    """
    æ•´åˆåœ‹æœƒè­°å“¡è‚¡ç¥¨äº¤æ˜“è³‡æ–™çš„çµ±ä¸€ä»‹é¢
    åƒè€ƒ burd5/congress_stock_trading å°ˆæ¡ˆæ¶æ§‹
    
    è³‡æ–™ä¾†æºï¼š
    1. åƒè­°é™¢å®˜æ–¹ç¶²ç«™ (efdsearch.senate.gov) - ä½¿ç”¨ AJAX API æŠ“å–è¡¨æ ¼è³‡æ–™
    2. çœ¾è­°é™¢å®˜æ–¹ç¶²ç«™ (disclosures-clerk.house.gov) - éœ€è§£æ PDF æ–‡ä»¶
    """
    
    # å®˜æ–¹ç¶²ç«™
    SENATE_BASE_URL = "https://efdsearch.senate.gov"
    HOUSE_BASE_URL = "https://disclosures-clerk.house.gov"
    
    def __init__(self, db_path=None):
        """
        åˆå§‹åŒ– Congress Trading Fetcher
        
        Args:
            db_path: SQLite è³‡æ–™åº«è·¯å¾‘ï¼Œé è¨­ä½¿ç”¨ database.py ä¸­çš„ DB_PATH
        """
        self.db_path = db_path or DB_PATH
        self.data = pd.DataFrame()
        self.init_db()
        
    def init_db(self):
        """åˆå§‹åŒ–è³‡æ–™åº«è¡¨æ ¼"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        # å»ºç«‹åœ‹æœƒäº¤æ˜“ä¸»è¡¨
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS congress_trades (
                record_id TEXT PRIMARY KEY,
                chamber TEXT NOT NULL,
                name TEXT NOT NULL,
                ticker TEXT,
                transaction_date TEXT,
                disclosure_date TEXT,
                transaction_type TEXT,
                amount TEXT,
                asset_description TEXT,
                owner TEXT,
                report_url TEXT,
                created_at TEXT DEFAULT CURRENT_TIMESTAMP,
                data_source TEXT
            )
        ''')
        
        # å»ºç«‹åƒè­°é™¢å ±å‘Šè¡¨
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS senate_reports (
                report_id TEXT PRIMARY KEY,
                senator_name TEXT,
                report_type TEXT,
                filing_date TEXT,
                report_url TEXT,
                is_processed INTEGER DEFAULT 0,
                created_at TEXT DEFAULT CURRENT_TIMESTAMP
            )
        ''')
        
        # å»ºç«‹çœ¾è­°é™¢å ±å‘Šè¡¨
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS house_reports (
                doc_id TEXT PRIMARY KEY,
                representative_name TEXT,
                filing_type TEXT,
                filing_date TEXT,
                year INTEGER,
                pdf_url TEXT,
                is_processed INTEGER DEFAULT 0,
                created_at TEXT DEFAULT CURRENT_TIMESTAMP
            )
        ''')
        
        conn.commit()
        conn.close()
        logger.info(f"Database initialized at {self.db_path}")
    
    def fetch_senate_transactions(self, start_date=None, end_date=None, days_back=30):
        """
        å¾åƒè­°é™¢å®˜æ–¹ç¶²ç«™æŠ“å–äº¤æ˜“è³‡æ–™ï¼ˆä½¿ç”¨ AJAX APIï¼‰
        åƒè€ƒ burd5/congress_stock_trading çš„ Senate scraper
        
        Args:
            start_date: é–‹å§‹æ—¥æœŸ (MM/DD/YYYY)
            end_date: çµæŸæ—¥æœŸ (MM/DD/YYYY)
            days_back: å¦‚æœæœªæŒ‡å®šæ—¥æœŸï¼Œå¾€å‰æŠ“å–çš„å¤©æ•¸
        
        Returns:
            bool: æ˜¯å¦æˆåŠŸ
        """
        logger.info("Fetching Senate transactions from official website...")
        
        # å¦‚æœæœªæŒ‡å®šæ—¥æœŸï¼Œä½¿ç”¨ days_back
        if not start_date or not end_date:
            end_dt = datetime.now()
            start_dt = end_dt - timedelta(days=days_back)
            start_date = start_dt.strftime("%m/%d/%Y")
            end_date = end_dt.strftime("%m/%d/%Y")
        
        try:
            from senate_fetcher_v1 import SenateFetcherV1
            
            fetcher = SenateFetcherV1()
            reports = fetcher.fetch_report_list(start_date, end_date)
            
            if reports:
                logger.info(f"âœ“ Found {len(reports)} Senate reports")
                
                # è½‰æ›ç‚º DataFrame
                senate_df = pd.DataFrame(reports)
                senate_df['chamber'] = 'Senate'
                senate_df['data_source'] = 'senate_official'
                
                self.data = senate_df
                return True
            else:
                logger.warning("No Senate reports found")
                return False
                
        except Exception as e:
            logger.error(f"Failed to fetch Senate data: {e}")
            import traceback
            traceback.print_exc()
            return False
    
    def fetch_senate_official(self, start_date=None, end_date=None, days_back=30):
        """
        å¾åƒè­°é™¢å®˜æ–¹ç¶²ç«™ç²å–æœ€æ–°è³‡æ–™
        
        Args:
            start_date: é–‹å§‹æ—¥æœŸ (MM/DD/YYYY æ ¼å¼)
            end_date: çµæŸæ—¥æœŸ (MM/DD/YYYY æ ¼å¼)
            days_back: å¦‚æœæœªæŒ‡å®šæ—¥æœŸï¼Œå¾€å‰æŠ“å–çš„å¤©æ•¸
        
        Returns:
            pd.DataFrame: åƒè­°é™¢äº¤æ˜“è³‡æ–™
        """
        logger.info("Fetching latest data from Senate official website...")
        
        # å¦‚æœæœªæŒ‡å®šæ—¥æœŸï¼Œä½¿ç”¨ days_back
        if not start_date or not end_date:
            end_dt = datetime.now()
            start_dt = end_dt - timedelta(days=days_back)
            start_date = start_dt.strftime("%m/%d/%Y")
            end_date = end_dt.strftime("%m/%d/%Y")
        
        try:
            from senate_fetcher_v1 import SenateFetcherV1
            house_transactions(self, year=None):
        """
        å¾çœ¾è­°é™¢å®˜æ–¹ç¶²ç«™æŠ“å–äº¤æ˜“è³‡æ–™ï¼ˆéœ€è§£æ PDFï¼‰
        åƒè€ƒ burd5/congress_stock_trading çš„ House scraper
        
        Args:
            year: å¹´ä»½ï¼Œé è¨­ç‚ºç•¶å‰å¹´ä»½
        
        Returns:
            bool: æ˜¯å¦æˆåŠŸ
        """
        if not year:
            year = datetime.now().year
        
        logger.info(f"Fetching House transactions for {year} from official website...")
        logger.info("Note: House data requires PDF parsing, this may take a while...")
        
        try:
            from house_fetcher_v3_ajax import HouseAjaxFetcher
            
            fetcher = HouseAjaxFetcher(self.db_path)
            fetcher.fetch_latest()
            
            # å¾è³‡æ–™åº«è®€å–è³‡æ–™
            conn = sqlite3.connect(self.db_path)
            house_df = pd.read_sql_query(
                "SELECT * FROM house_reports WHERE year = ? ORDER BY filing_date DESC",
                conn,
                params=(year,)
            )
            conn.close()
            
            if not house_df.empty:
                house_df['chamber'] = 'House'
                house_df['data_source'] = 'house_official'
                logger.info(f"âœ“ Found {len(house_df)} House reports")
                
                if self.data.empty:
                    self.data = house_df
                else:
                    self.data = pd.concat([self.data, house_df], ignore_index=True)
                return True
            else:
                logger.warning(f"No House reports found for {year}")
                return False
                
        except Exception as e:
            logger.error(f"Failed to fetch House data: {e}")
            import traceback
            traceback.print_exc()
            return False
    
    def fetch_all(self, days_back=30, include_house=True):
        """
        å¾å®˜æ–¹ç¶²ç«™æŠ“å–æ‰€æœ‰è³‡æ–™
        
        Args:
            days_back: åƒè­°é™¢è³‡æ–™å›æº¯å¤©æ•¸
            include_house: æ˜¯å¦åŒ…å«çœ¾è­°é™¢ï¼ˆæœƒæ¯”è¼ƒæ…¢ï¼Œå› ç‚ºéœ€è¦è™•ç† PDFï¼‰
        
        Returns:
            bool: æ˜¯å¦æˆåŠŸ
        """
        logger.info("=" * 60)
        logger.info("ğŸ”¥ Congress Trading Fetcher - Scraping Official Sources")
        logger.info("=" * 60)
        
        success = False
        
        # æŠ“å–åƒè­°é™¢è³‡æ–™
        logger.info("\n[1/2] Fetching Senate data...")
        if self.fetch_senate_transactions(days_back=days_back):
            success = True
        
        # æŠ“å–çœ¾è­°é™¢è³‡æ–™
        if include_house:
            logger.info("\n[2/2] Fetching House data...")
            if self.fetch_house_transactions():
                success = True
        
        if success and not self.data.empty:
            # å»é‡
            if 'record_id' in self.data.columns:
                original_count = len(self.data)
                self.data = self.data.drop_duplicates(subset=['record_id'])
                removed = original_count - len(self.data)
                if removed > 0:
                    logger.info(f"Removed {removed} duplicate records"
        if 'record_id' not in df.columns:
            df['record_id'] = df.apply(
                lambda row: generate_hash(f"{row.get('name', '')}_{row.get('ticker', '')}_{row.get('transaction_date', '')}"),
                axis=1
            )
        
        # é¸æ“‡éœ€è¦çš„æ¬„ä½
        columns = ['record_id', 'chamber', 'name', 'ticker', 'transaction_date', 
                   'disclosure_date', 'transaction_type', 'amount', 'asset_description',
                   'owner', 'report_url', 'data_source']
        
        # åªä¿ç•™å­˜åœ¨çš„æ¬„ä½
        available_columns = [col for col in columns if col in df.columns]
        df_to_save = df[available_columns]
        
        # å„²å­˜åˆ°è³‡æ–™åº« (ä½¿ç”¨ replace é¿å…é‡è¤‡)
        df_to_save.to_sql('congress_trades', conn, if_exists='append', index=False)
        
        conn.commit()
        conn.close()
        
        logger.info(f"âœ“ {len(df_to_save)} records saved to database")
        return len(df_to_save)
    
    def analyze(self, days=30, top_n=20):
        """
        åˆ†æäº¤æ˜“è³‡æ–™
        
        Args:
            days: åˆ†ææœ€è¿‘å¹¾å¤©çš„è³‡æ–™
            top_n: é¡¯ç¤ºå‰ N å
        """
        if self.data.empty:
            logger.warning("No data to analyze")
            return
        
        df = self.data.copy()
        
        # è½‰æ›æ—¥æœŸæ¬„ä½
        date_col = 'disclosure_date' if 'disclosure_date' in df.columns else 'transaction_date'
        if date_col in df.columns:
            df[date_col] = pd.to_datetime(df[date_col], errors='coerce')
            df = df.dropna(subset=[date_col])
            
            # éæ¿¾æœ€è¿‘çš„è³‡æ–™
            recent_df = df[df[date_col] >= (datetime.now() - timedelta(days=days))]
        else:
            recent_df = df
        
        print("\n" + "ğŸ”¥" * 30)
        print(f" LATEST CONGRESSIONAL TRADES (Last {days} Days)")
        print("ğŸ”¥" * 30)
        
        if not recent_df.empty:
            # é¡¯ç¤ºæ¬„ä½ï¼ˆæ ¹æ“šå¯ç”¨æ¬„ä½èª¿æ•´ï¼‰
            display_cols = []
            for col in [date_col, 'name', 'ticker', 'transaction_type', 'amount', 'chamber', 'data_source']:
                if col in recent_df.columns:
                    display_cols.append(col)
            
            latest = recent_df.sort_values(by=date_col, ascending=False)
            print(latest[display_cols].head(top_n).to_string(index=False))
        else:
            print(f"No records found in the last {days} days.")
        
        # ç†±é–€è‚¡ç¥¨çµ±è¨ˆ
        if 'ticker' in recent_df.columns:
            print("\n" + "ğŸ’" * 30)
            print(" TOP TICKERS BY DISCLOSURE COUNT")
            print("ğŸ’" * 30)
            ticker_counts = recent_df['ticker'].value_counts().head(10)
            print(ticker_counts.to_string())
        
        # æŒ‰è­°å“¡çµ±è¨ˆ
        if 'name' in recent_df.columns:
            print("\n" + "ğŸ‘¥" * 30)
            print(" TOP TRADERS BY TRANSACTION COUNT")
            print("ğŸ‘¥" * 30)
            trader_counts = recent_df['name'].value_counts().head(10)
            print(trader_counts.to_string())
        
        # äº¤æ˜“é¡å‹çµ±è¨ˆ
        if 'transaction_type' in recent_df.columns:
            print("\n" + "ğŸ“Š" * 30)
            print(" TRANSACTION TYPES BREAKDOWN")
            print("ğŸ“Š" * 30)
            type_counts = recent_df['transaction_type'].value_counts()
            print(type_counts.to_string())
    
    def get_trades_by_ticker(self, ticker, days=90):
        """
        ç²å–ç‰¹å®šè‚¡ç¥¨çš„æ‰€æœ‰äº¤æ˜“ç´€éŒ„
        
        Args:
            ticker: è‚¡ç¥¨ä»£ç¢¼
            days: å›æº¯å¤©æ•¸
        
        Returns:
            pd.DataFrame: äº¤æ˜“ç´€éŒ„
        """
        if self.data.empty:
            return pd.DataFrame()
        
        df = self.data.copy()
        
        # éæ¿¾è‚¡ç¥¨ä»£ç¢¼
        if 'ticker' in df.columns:
            ticker_df = df[df['ticker'].str.upper() == ticker.upper()]
            
            # éæ¿¾æ—¥æœŸ
            date_col = 'disclosure_date' if 'disclosure_date' in ticker_df.columns else 'transaction_date'
            if date_col in ticker_df.columns:
                ticker_df[date_col] = pd.to_datetime(ticker_df[date_col], errors='coerce')
                ticker_df = ticker_df[ticker_df[date_col] >= (datetime.now() - timedelta(days=days))]
                ticker_df = ticker_df.sort_values(by=date_col, ascending=False)
            
            return ticker_df
        
        return pd.DataFrame()
    
    def get_trades_by_politician(self, name, days=180):
        """
        ç²å–ç‰¹å®šè­°å“¡çš„æ‰€æœ‰äº¤æ˜“ç´€éŒ„
        
        Args:
            name: è­°å“¡å§“åï¼ˆéƒ¨åˆ†åŒ¹é…ï¼‰
            days: å›æº¯å¤©æ•¸
        
        Returns:
            pd.DataFrame: äº¤æ˜“ç´€éŒ„
        """
        if self.data.empty:
            return pd.DataFrame()
        
        df = self.data.copy()
        
        # éæ¿¾è­°å“¡å§“å
        if 'name' in df.columns:
            politician_df = df[df['name'].str.contains(name, case=False, na=False)]
            
            # éæ¿¾æ—¥æœŸ
            date_col = 'disclosure_date' if 'disclosure_date' in politician_df.columns else 'transaction_date'
            if date_col in politician_df.columns:
                politician_df[date_col] = pd.to_datetime(politician_df[date_col], errors='coerce')
                politician_df = politician_df[politician_df[date_col] >= (datetime.now() - timedelta(days=days))]
                politician_df = politician_df.sort_values(by=date_col, ascending=False)
            
            return politician_df
        
        return pd.DataFrame()


def main():
    """ä¸»ç¨‹å¼ï¼šç¤ºç¯„å¦‚ä½•ä½¿ç”¨"""
    print("\n" + "=" * 60)
    print("Congress Trading Fetcher - Demo")
    print("=" * 60 + "\n")
    
    # åˆå§‹åŒ–
    fetcher = CongressTradingFetcher()
    
    # æ–¹æ¡ˆ 1ï¼šåªç²å– GitHub æ­·å²è³‡æ–™ï¼ˆæœ€å¿«ï¼‰
    print("\n[Option 1] Fetching from GitHub (Fast)...")
    if fetcher.fetch_from_github('both'):
        fetcher.analyze(days=60, top_n=15)
    
    # æ–¹æ¡ˆ 2ï¼šç²å–æ‰€æœ‰è³‡æ–™æºï¼ˆåŒ…å«å®˜æ–¹æœ€æ–°è³‡æ–™ï¼‰
    # æ³¨æ„ï¼šé€™æœƒæ¯”è¼ƒæ…¢ï¼Œå› ç‚ºè¦æŠ“å–å®˜æ–¹ç¶²ç«™
    # print("\n[Option 2] Fetching from all sources (Slow but complete)...")
    # if fetcher.fetch_all_sources(include_official=True, days_back=30):
    #     fetcher.save_to_database()
    #     fetcher.analyze(days=30, top_n=15)
    
    # æ–¹æ¡ˆ 3ï¼šæŸ¥è©¢ç‰¹å®šè‚¡ç¥¨
    print("\n[Option 3] Search by ticker (NVDA)...")
    nvda_trades = fetcher.get_trades_by_ticker('NVDA', days=90)
    if not nvda_trades.empty:
        print(f"\nFound {len(nvda_trades)} NVDA trades:")
        display_cols = [col for col in ['transaction_date', 'name', 'transaction_type', 'amount', 'chamber'] 
                       if col in nvda_trades.columns]
        print(nvda_trades[display_cols].head(10).to_string(index=False))
    
    # æ–¹æ¡ˆ 4ï¼šæŸ¥è©¢ç‰¹å®šè­°å“¡
    print("\n[Option 4] Search by politician (Pelosi)...")
    pelosi_trades = fetcher.get_trades_by_politician('Pelosi', days=180)
    if not pelosi_trades.empty:
        print(f"\nFound {len(pelosi_trades)} trades by Pelosi:")
        display_cols = [col for col in ['transaction_date', 'ticker', 'transaction_type', 'amount'] 
                       if col in pelosi_trades.columns]
        print(pelosi_trades[display_cols].head(10).to_string(index=False))


if __name__ == "__main__":
    main()
æŠ“å–åƒè­°é™¢æœ€è¿‘ 30 å¤©è³‡æ–™ï¼ˆæ¨è–¦ï¼Œè¼ƒå¿«ï¼‰
    print("\n[Option 1] Scraping Senate data (30 days)...")
    if fetcher.fetch_senate_transactions(days_back=30):
        fetcher.save_to_database()
        fetcher.analyze(days=30, top_n=15)
    
    # æ–¹æ¡ˆ 2ï¼šæŠ“å–æ‰€æœ‰è³‡æ–™ï¼ˆåƒè­°é™¢ + çœ¾è­°é™¢ï¼‰
    # æ³¨æ„ï¼šçœ¾è­°é™¢éœ€è¦è™•ç† PDFï¼Œæœƒæ¯”è¼ƒæ…¢
    # print("\n[Option 2] Scraping all sources...")
    # if fetcher.fetch_all(days_back=30, include_house=True):
    #     fetcher.save_to_database()
    #     fetcher.analyze(days=30, top_n=15